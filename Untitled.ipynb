{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>adjusted_price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>maximum_nights</th>\n",
       "      <th>ColumnDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>736534</td>\n",
       "      <td>12/29/2022</td>\n",
       "      <td>f</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>736534</td>\n",
       "      <td>12/30/2022</td>\n",
       "      <td>f</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>736534</td>\n",
       "      <td>12/31/2022</td>\n",
       "      <td>f</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>736534</td>\n",
       "      <td>1/1/2023</td>\n",
       "      <td>f</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>736534</td>\n",
       "      <td>1/2/2023</td>\n",
       "      <td>f</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available   price  adjusted_price  minimum_nights  \\\n",
       "0      736534  12/29/2022         f  1250.0          1250.0               1   \n",
       "1      736534  12/30/2022         f  1250.0          1250.0               2   \n",
       "2      736534  12/31/2022         f  1250.0          1250.0               2   \n",
       "3      736534    1/1/2023         f  1250.0          1250.0               1   \n",
       "4      736534    1/2/2023         f  1250.0          1250.0               1   \n",
       "\n",
       "   maximum_nights  ColumnDiff  \n",
       "0            1125         0.0  \n",
       "1            1125         0.0  \n",
       "2            1125         0.0  \n",
       "3            1125         0.0  \n",
       "4            1125         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_df_cleaned = pd.read_csv(\"cal_df_cleaned.csv\")\n",
    "cal_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation Approach\n",
    "\n",
    "For this project, we aim at creating a recommender system that will benefit the stakeholders in the following ways:\n",
    "1. Increasing Customer Engagement: Recommender systems can be used to increase customer engagement by providing personalized recommendations based on their preferences, behavior, and historical data. By improving the quality of recommendations, businesses can keep their customers engaged and increase their retention rate.\n",
    "\n",
    "2. Improving Customer Experience: A good recommender system can help improve the customer experience by suggesting relevant products or services that meet their needs and expectations. By delivering personalized recommendations, businesses can enhance their customers' satisfaction, leading to increased loyalty and repeat business.\n",
    "\n",
    "3. Boosting Sales: Recommender systems can also help boost sales by increasing cross-selling and up-selling opportunities. By suggesting complementary products or services, businesses can encourage customers to make additional purchases, increasing their revenue and profit margins.\n",
    "\n",
    "4. Reducing Churn Rate: Recommender systems can help reduce the churn rate by providing targeted recommendations that keep customers interested in the business. By recommending products or services that align with the customers' interests and preferences, businesses can decrease the likelihood of losing customers to competitors.\n",
    "\n",
    "5. Improving Operational Efficiency: Recommender systems can help improve operational efficiency by automating the recommendation process and reducing the need for manual intervention. By using machine learning algorithms, businesses can streamline their operations and save time and resources, leading to increased productivity and profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "User Preferences: Collect user preferences on location, type of accommodation, price range, and other relevant criteria that are important to the user.\n",
    "\n",
    "User Behavior: Analyze user behavior within the Airbnb platform, such as the types of properties they have booked, the frequency of bookings, and the reviews they have left for properties.\n",
    "\n",
    "Property Characteristics: Consider the characteristics of the properties, such as the location, property type, size, amenities, and other relevant features that may influence the user's decision.\n",
    "\n",
    "Reviews and Ratings: Analyze the reviews and ratings left by previous guests for each property to determine their overall satisfaction and identify any common issues or concerns.\n",
    "\n",
    "External Data: Incorporate external data sources, such as weather forecasts, events happening in the area, and local transportation options, to provide additional context for the user.\n",
    "\n",
    "Similarity Matching: Use algorithms that find properties that are similar to the ones the user has shown interest in, based on factors like location, property type, price range, and other relevant criteria.\n",
    "\n",
    "Personalization: Tailor recommendations to the user's unique preferences and behavior, by using machine learning algorithms that can learn and adapt over time.\n",
    "\n",
    "Diversity and Serendipity: Provide recommendations that are diverse and offer unexpected options, to help the user discover new and exciting properties they may not have considered before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based filtering Recommmender System using Count Vectorizer\n",
    "\n",
    "Content-based filtering technique recommends items similar to what the user has already shown interest in. In this project, we can recommend listings similar to the one the user has already viewed. To do this, we can use the listing's textual attributes, such as the name, description, and amenities, neighbourhood_overview, price and review_scores_rating to calculate similarity between listings.\n",
    "\n",
    "\n",
    "The count vectorizer is a common technique used to convert text data into numerical features that can be used for recommendation. It involves representing each document or piece of text as a vector of word counts, where each dimension corresponds to a unique word in the vocabulary, and the value in each dimension is the count of how many times that word appears in the document. \n",
    "\n",
    "For this, we will use relevant features to check the count of texts(which will be a combination of a variety of relevant features)to group similar users usig Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the data\n",
    "df = listing_df_cleaned\n",
    "\n",
    "\n",
    "# Select the relevant features\n",
    "features = ['id', 'name', 'description', 'neighborhood_overview', 'price', 'review_scores_rating', \n",
    "            'property_type', 'amenities']\n",
    "\n",
    "# Create a new DataFrame with only the relevant features\n",
    "df_relevant = df[features]\n",
    "\n",
    "# Preprocess the text data\n",
    "# by combining the name, description, and neighborhood overview columns into a single text column.\n",
    "df_relevant['text'] = df_relevant['name'] + ' ' + df_relevant['description'] + ' ' + df_relevant['neighborhood_overview']\n",
    "\n",
    "# Create a CountVectorizer object to convert the text data into a matrix of word counts.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer to the text data\n",
    "X = vectorizer.fit_transform(df_relevant['text'])\n",
    "\n",
    "# Calculate the cosine similarity between all listings based on the matrix of word counts\n",
    "cosine_similarities = cosine_similarity(X)\n",
    "\n",
    "# Define a function to get the most similar listings\n",
    "#This function takes the listing ID as an argument and \n",
    "# returns a DataFrame containing the top n most similar listings based on cosine similarity.\n",
    "def get_similar_listings(listing_id, n=10):\n",
    "    # Get the row index for the given listing ID\n",
    "    idx = df_relevant[df_relevant['id'] == listing_id].index[0]\n",
    "    \n",
    "    # Get the cosine similarities for the given row\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    \n",
    "    # Sort the list of similarities by score in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the indices of the top n similar listings\n",
    "    sim_indices = [i[0] for i in sim_scores[1:n+1]]\n",
    "    \n",
    "    # Return the top n similar listings\n",
    "    return df_relevant.iloc[sim_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use the recommender system, you would simply call the get_similar_listings function with a listing ID as the argument.\n",
    "# For example, to get the top 10 most similar listings to a listing with ID 12345, you would call the function like this\n",
    "get_similar_listings(3191, n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code, we see that the Airbnb Listing_Id = 3191, has a similar number of clients and thus implementing this recommender system will help reduce the diversification of recommendations that pop-up at these clients's Airbnb Platform/ emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering based Recommender System using SVD\n",
    "\n",
    "Collaborative filtering is a type of recommender system that uses the past behavior and preferences of users to make recommendations. The underlying idea is that users who have similar tastes and preferences in the past are likely to have similar tastes and preferences in the future.\n",
    "\n",
    "Collaborative filtering works by creating a user-item matrix that represents the ratings or preferences of users for various items. This matrix is then used to predict the ratings or preferences of users for items that they have not yet interacted with. This is done by finding users who have similar ratings or preferences as the target user, and then recommending items that those similar users have liked or rated highly.\n",
    "\n",
    "There are two main types of collaborative filtering: user-based and item-based. User-based collaborative filtering recommends items to a target user based on the ratings and preferences of users who are similar to the target user. Item-based collaborative filtering recommends items to a target user based on the similarity of the items they have rated or interacted with in the past.\n",
    "\n",
    "Collaborative filtering has some advantages over other types of recommender systems, such as content-based systems. Collaborative filtering can recommend items that are outside the user's usual preferences, but still likely to be enjoyed based on the preferences of similar users. It also works well in situations where there is limited information about the items being recommended, such as for new or niche products. However, collaborative filtering also has some limitations, such as the cold-start problem, where it is difficult to make recommendations for new users who have not yet rated any items, and the sparsity problem, where there may be many users and items with very few ratings, making it difficult to find similar users or items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Based Collaborative Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df_cleaned['review_scores_rating'] = listing_df_cleaned['review_scores_rating'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "\n",
    "#Create a user-item matrix:\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(listing_df_cleaned[['id', 'host_id', 'review_scores_rating']], reader)\n",
    "\n",
    "train_data = data.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the user-based collaborative filtering model:\n",
    "from surprise import KNNWithMeans\n",
    "\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "algo.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Predictions for a user\n",
    "# Get the top-n recommendations for a user\n",
    "def ubcf_recommendations(user_id, top_n=10):\n",
    "    items = df['id'].unique()\n",
    "    user_items = df[df['host_id'] == user_id]['id'].tolist()\n",
    "    other_items = [item for item in items if item not in user_items]\n",
    "    \n",
    "    predictions = []\n",
    "    for item_id in other_items:\n",
    "        predictions.append((item_id, algo.predict(user_id, item_id).est))\n",
    "    \n",
    "    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    recommended_items = [item[0] for item in recommendations]\n",
    "    return recommended_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make predictions now:\n",
    "user_id = 3754\n",
    "recommended_items = ubcf_recommendations(user_id, top_n=5)\n",
    "\n",
    "print(f\"Top 5 recommended items for user {user_id}:\")\n",
    "for item_id in recommended_items:\n",
    "    print(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Perfomance of the User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Define the reader and load the data into the Surprise dataset format\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "data = Dataset.load_from_df(listing_df_cleaned[['host_id', 'id', 'review_scores_rating']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train the algorithm on the training set\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Compute and print the RMSE and MAE scores for the predictions\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "print('Root Mean Squared Error (RMSE):', rmse)\n",
    "print('Mean Absolute Error (MAE):', mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code, you should see the RMSE score for the predictions. This score represents the root mean squared error of the difference between the predicted ratings and the actual ratings in the test set. The lower the RMSE score, the better the performance of the collaborative filtering algorithm.Thus we can use this user-based model to make a personalized recommendations for users based on their past ratings and behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time Series Analysis using SARIMA, Prophet and Seasonal Decomposition Time Series(STL mODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA : Seasonal ARIMA (SARIMA) model: SARIMA models are commonly used to analyze time series data with seasonal patterns. SARIMA models are a type of ARIMA model that takes into account the seasonal component of the data. This model can be used to capture trends, seasonality, and cyclical behavior in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = cal_df_cleaned1\n",
    "\n",
    "# Set the date column as the index\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "calendar_df = cal_df_cleaned.copy()\n",
    "# Preprocess the data\n",
    "calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
    "calendar_df = calendar_df.dropna(subset=['price'])\n",
    "\n",
    "# Group the data by date and calculate the average price\n",
    "avg_price_df = calendar_df.groupby('date')['price'].mean()\n",
    "\n",
    "# Plot the time series\n",
    "plt.plot(avg_price_df)\n",
    "plt.title('Average price over time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n",
    "\n",
    "# Decompose the time series to observe its trend and seasonality\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(avg_price_df)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(avg_price_df, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Fit a SARIMAX model to the time series\n",
    "model = SARIMAX(avg_price_df, order=(1, 1, 1), seasonal_order=(0, 1, 1, 12))\n",
    "results = model.fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(results.summary())\n",
    "\n",
    "# Make predictions with the model\n",
    "forecast = results.forecast(steps=12)\n",
    "print(forecast)\n",
    "\n",
    "# Plot the actual and predicted values\n",
    "plt.plot(avg_price_df, label='Actual')\n",
    "plt.plot(forecast, label='Predicted')\n",
    "plt.title('Actual vs Predicted Average Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > Dep. Variable: The dependent variable in the model, which in this case is \"price\".\n",
    "* > No. Observations: The number of observations used in the model.\n",
    "* > Model: The specifications of the SARIMA model used in the analysis, which is (1, 1, 1)x(0, 1, 1, 12).\n",
    "* > Log Likelihood: The log likelihood of the estimated model parameters.\n",
    "* > AIC, BIC, and HQIC: The Akaike Information Criterion, Bayesian Information Criterion, and Hannan-Quinn Information Criterion, respectively. These are used to compare different models and select the best one based on the lowest value.\n",
    "* > Covariance Type: The method used to estimate the covariance matrix, which in this case is \"opg\".\n",
    "* > Coefficients: The estimated coefficients for the SARIMA model. In this case, there are three coefficients: ar.L1, ma.L1, and ma.S.L12. These represent the autoregressive, moving average, and seasonal moving average parameters, respectively.\n",
    "* > sigma2: The estimated variance of the error term in the model.\n",
    "* > Ljung-Box (L1) (Q): A test for autocorrelation in the residuals at a lag of 1. A p-value greater than 0.05 indicates that there is no evidence of autocorrelation.\n",
    "* > Jarque-Bera (JB): A test for normality of the residuals. A p-value less than 0.05 indicates that the residuals are not normally distributed.\n",
    "* > Heteroskedasticity (H): A test for heteroscedasticity in the residuals. A p-value less than 0.05 indicates that the residuals are heteroscedastic.\n",
    "* > Warnings: Any warnings or notes about the model estimation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Load Airbnb data for Cape Town\n",
    "df = cal_df_cleaned.copy()\n",
    "\n",
    "\n",
    "# Create a SARIMA model with seasonal_order=(1,1,1,12)\n",
    "model = SARIMAX(df['price'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "\n",
    "# Fit the model to the data\n",
    "results = model.fit()\n",
    "\n",
    "# Make predictions for the next 12 months\n",
    "forecast = results.predict(start=len(df), end=len(df)+11, dynamic=False)\n",
    "\n",
    "# Plot the forecast\n",
    "forecast.plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet model: Prophet is a time series forecasting model developed by Facebook. It is particularly well-suited for data that have seasonality and multiple seasonalities with changing trends. The model uses an additive approach to model seasonality, trends, and holidays in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df_cleaned.copy().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = cal_df_cleaned.copy()\n",
    "\n",
    "# Rename columns to \"ds\" and \"y\"\n",
    "df = df.rename(columns={'date': 'ds', 'price': 'y'})\n",
    "\n",
    "\n",
    "# Instantiate Prophet model and fit to data\n",
    "model = Prophet()\n",
    "model.fit(df)\n",
    "# Generate future dates for forecasting\n",
    "future_dates = model.make_future_dataframe(periods=365)\n",
    "\n",
    "# Make predictions for future dates\n",
    "forecast = model.predict(future_dates)\n",
    "\n",
    "# Plot forecast\n",
    "model.plot(forecast, xlabel='Date', ylabel='Price')\n",
    "\n",
    "# Plot components of forecast (trend, seasonality, holidays)\n",
    "model.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Seasonality: In a time series, seasonality refers to patterns that repeat at fixed intervals of time, such as weekly, monthly, or yearly. In a Prophet model, seasonality is represented by a set of smooth curves that show how the time series varies with the season. These curves are shown in the \"seasonality\" plot that is produced by model.plot_components(). You can use this plot to identify the periods of the year when prices tend to be high or low, and to see how the seasonality changes over time.\n",
    "\n",
    "> * Trends: In a time series, trends refer to long-term patterns that are not related to seasonality. Trends can be upward or downward, and may be linear or nonlinear. In a Prophet model, trends are represented by a smooth curve that shows how the time series is changing over time. This curve is shown in the \"trend\" plot that is produced by model.plot_components(). You can use this plot to see whether prices are generally increasing or decreasing over time, and to identify any sudden changes in the trend.\n",
    "\n",
    ">* Patterns: In a time series, patterns refer to any other features that are not related to seasonality or trend, such as sudden spikes or drops in the data. In a Prophet model, patterns are represented by a combination of trend and seasonality, and are captured by the residuals of the model. These residuals can be visualized using the \"residuals\" plot that is produced by model.plot()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal Decomposition of Time Series (STL) model: The STL model is a classical decomposition method that can be used to identify seasonality in a time series. The model decomposes the time series into three components: trend, seasonal, and residual. The seasonal component can be used to identify seasonal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Load Airbnb data for Cape Town\n",
    "df = cal_df_cleaned.copy()\n",
    "\n",
    "# Set the date column as the index\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Create an STL decomposition model\n",
    "model = STL(df['price'], period=12)\n",
    "\n",
    "# Fit the model to the data\n",
    "results = model.fit()\n",
    "\n",
    "# Plot the seasonal component of the data\n",
    "results.seasonal.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load Airbnb data for Cape Town\n",
    "df = cal_df_cleaned.copy()\n",
    "\n",
    "# Set the date column as the index\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train = df.iloc[:-12]\n",
    "test = df.iloc[-12:]\n",
    "\n",
    "# Create a SARIMA model with seasonal_order=(1,1,1,12)\n",
    "sarima_model = SARIMAX(train['price'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "sarima_results = sarima_model.fit()\n",
    "\n",
    "# Create a Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(train.reset_index().rename(columns={'date':'ds', 'price':'y'}))\n",
    "\n",
    "# Create an STL decomposition model\n",
    "stl_model = STL(train['price'], period=12)\n",
    "stl_results = stl_model.fit()\n",
    "\n",
    "# Make predictions using each model\n",
    "sarima_preds = sarima_results.predict(start=len(train), end=len(train)+11, dynamic=False)\n",
    "prophet_preds = prophet_model.make_future_dataframe(periods=12, freq='M')\n",
    "prophet_preds = prophet_model.predict(prophet_preds)[-12:]['yhat']\n",
    "stl_preds = stl_results.seasonal[-12:].values\n",
    "\n",
    "# Calculate RMSE for each model\n",
    "sarima_rmse = np.sqrt(mean_squared_error(test['price'], sarima_preds))\n",
    "prophet_rmse = np.sqrt(mean_squared_error(test['price'], prophet_preds))\n",
    "stl_rmse = np.sqrt(mean_squared_error(test['price'], stl_preds))\n",
    "\n",
    "# Print RMSE for each model\n",
    "print('SARIMA RMSE: ', sarima_rmse)\n",
    "print('Prophet RMSE: ', prophet_rmse)\n",
    "print('STL RMSE: ', stl_rmse)\n",
    "\n",
    "# Choose the model with the lowest RMSE\n",
    "if sarima_rmse <= prophet_rmse and sarima_rmse <= stl_rmse:\n",
    "    print('SARIMA is the best model.')\n",
    "elif prophet_rmse <= sarima_rmse and prophet_rmse <= stl_rmse:\n",
    "    print('Prophet is the best model.')\n",
    "else:\n",
    "    print('STL is the best model.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # HYPERPARAMETER TUNING FOR SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
